
from __future__ import print_function 
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from pyspark import SparkContext
import argparse
import time
import json


broker = 'localhost:9092'
topic = 'DDOS_detect'
sc = SparkContext(appName="DetectDDOS")
ssc = StreamingContext(sc, 1)
ssc.checkpoint('checkpoint')
kvs = KafkaUtils.createDirectStream(ssc, [topic], {"metadata.broker.list": broker})


def get_message(text):
    message = json.loads(text[1])
    return message['remote_host'], 1

def new_freq(entry, frequency):
  if not frequency:
    frequency = 0
  return sum(entry) + frequency


log_file = kvs.map(get_message)  
log1 = log_file.updateStateByKey(new_freq)
log2 = log1.map(lambda (k,v): (str(k), v))
suspect = log2.filter(lambda (k,v): v >= 4)
suspect.pprint()
suspect.saveAsTextFiles('DDOS_attacker')
ssc.start()
time.sleep(30)
ssc.stop()
ssc.awaitTermination()


